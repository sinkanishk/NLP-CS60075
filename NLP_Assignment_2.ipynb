{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zyJ25uz0kSaw"
   },
   "source": [
    "# Assignment 2 on Natural Language Processing\n",
    "\n",
    "### Date : 15th Sept, 2020\n",
    "\n",
    "### Instructor : Prof. Sudeshna Sarkar\n",
    "\n",
    "### Teaching Assistants : Alapan Kuila, Aniruddha Roy, Anusha Potnuru, Uppada Vishnu\n",
    "\n",
    "### Name - Kanishk Singh\n",
    "\n",
    "### Roll no. - 17CS30018"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ao1nhg9RknmF"
   },
   "source": [
    "The central idea of this assignment is to make you familiar with programming in python and also the language modelling task of natural language processing using the python.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "stk58juYkzEr"
   },
   "source": [
    "**Dataset:** \n",
    "\n",
    " Use the text file provided along."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/kanishk/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "import nltk.data\n",
    "import re\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rT6byv49kdmo"
   },
   "outputs": [],
   "source": [
    "filename = 'corpus.txt'\n",
    "with open(filename) as f:\n",
    "    content = f.readlines()\n",
    "# you may also want to remove whitespace characters like `\\n` at the end of each line\n",
    "content = [x.strip() for x in content] #removes whitespaces from the text input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SRGqKaDn1pJy"
   },
   "source": [
    "Preprocess the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "C1OtHn6B1oc2"
   },
   "outputs": [],
   "source": [
    "def remove_punct(data=None):\n",
    "   # punctuation marks \n",
    "    punctuations = '''!()-[]{};:'\"\\,”<>.“/?@#$%^&*_~'''\n",
    "    for x in data.lower(): \n",
    "        if x in punctuations: \n",
    "            data = data.replace(x, \"\")\n",
    "    return(data)\n",
    "def lower_case(data=None):\n",
    "    result = data.lower()\n",
    "    return result\n",
    "def remove_alphanumeric(data=None): \n",
    "    result = \"\" \n",
    "    for i in data: \n",
    "        # Store only valid characters \n",
    "        if (i >= 'A' and i <= 'Z') or (i >= 'a' and i <= 'z') or (i==' '): \n",
    "            result += i \n",
    "    return(result) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "content = [remove_punct(x) for x in content]\n",
    "content = [lower_case(x) for x in content]\n",
    "content = [remove_alphanumeric(x) for x in content]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YDL7yfpXkMRS"
   },
   "source": [
    "### Task: In this sub-task, you are expected to carry out the following tasks:\n",
    "\n",
    "1. **Create the following language models** on the training corpus: <br>\n",
    "    i.   Unigram <br>\n",
    "    ii.  Bigram <br>\n",
    "    iii. Trigram <br>\n",
    "    iv.  Fourgram <br>\n",
    "\n",
    "2. **List the top 5 bigrams, trigrams, four-grams (with and without Add-1 smoothing).**\n",
    "(Note: Please remove those which contain only articles, prepositions, determiners. For Example: “of the”, “in a”, etc)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "u3oIulBikPua"
   },
   "outputs": [],
   "source": [
    "#Write code\n",
    "import nltk\n",
    "from nltk.util import ngrams\n",
    "unigrams=[]\n",
    "bigrams=[]\n",
    "trigrams=[]\n",
    "fourgrams=[]\n",
    "for text in content:\n",
    "    unigrams.extend(ngrams(text.split(),1))\n",
    "    bigrams.extend(ngrams(text.split(),2))\n",
    "    trigrams.extend(ngrams(text.split(),3))\n",
    "    fourgrams.extend(ngrams(text.split(),4))\n",
    "    ##similar for trigrams and fourgrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The most common unigrams after removing stopwords without smoothing are \n",
      "('the',)   1630\n",
      "('and',)   844\n",
      "('to',)   721\n",
      "('a',)   627\n",
      "('she',)   537\n",
      "('it',)   526\n",
      "('of',)   508\n",
      "('said',)   462\n",
      "('i',)   400\n",
      "('alice',)   385\n",
      "The most common bigrams without removing stopwords are \n",
      "('said', 'the')   209\n",
      "('of', 'the')   130\n",
      "('said', 'alice')   115\n",
      "('in', 'a')   97\n",
      "('and', 'the')   80\n",
      "('in', 'the')   78\n",
      "('it', 'was')   74\n",
      "('to', 'the')   69\n",
      "('the', 'queen')   65\n",
      "('as', 'she')   61\n",
      "\n",
      "The most common trigrams without removing stopwords are \n",
      "('the', 'mock', 'turtle')   51\n",
      "('the', 'march', 'hare')   30\n",
      "('said', 'the', 'king')   29\n",
      "('the', 'white', 'rabbit')   21\n",
      "('said', 'the', 'hatter')   21\n",
      "('said', 'to', 'herself')   19\n",
      "('said', 'the', 'mock')   19\n",
      "('said', 'the', 'caterpillar')   18\n",
      "('she', 'went', 'on')   17\n",
      "('she', 'said', 'to')   17\n",
      "\n",
      "The most common fourgrams without removing stopwords are \n",
      "('said', 'the', 'mock', 'turtle')   19\n",
      "('she', 'said', 'to', 'herself')   16\n",
      "('a', 'minute', 'or', 'two')   11\n",
      "('said', 'the', 'march', 'hare')   8\n",
      "('will', 'you', 'wont', 'you')   8\n",
      "('said', 'alice', 'in', 'a')   7\n",
      "('as', 'well', 'as', 'she')   6\n",
      "('well', 'as', 'she', 'could')   6\n",
      "('in', 'a', 'great', 'hurry')   6\n",
      "('in', 'a', 'tone', 'of')   6\n"
     ]
    }
   ],
   "source": [
    "#print top 10 unigrams, bigrams without removing stopwords\n",
    "uni_processed = [p for p in unigrams]\n",
    "fdist = nltk.FreqDist(uni_processed)\n",
    "\n",
    "bi_processed = [p for p in bigrams]\n",
    "bidist = nltk.FreqDist(bi_processed)\n",
    "\n",
    "tri_processed = [p for p in trigrams]\n",
    "tridist = nltk.FreqDist(tri_processed)\n",
    "\n",
    "four_processed = [p for p in fourgrams]\n",
    "fourdist = nltk.FreqDist(four_processed)\n",
    "\n",
    "\n",
    "#print top 10 bigrams, trigrams, fourgrams without removing stopwords\n",
    "print('The most common unigrams after removing stopwords without smoothing are ')\n",
    "for word in fdist.most_common(10):\n",
    "    print(word[0],\" \",word[1])\n",
    "print('The most common bigrams without removing stopwords are ')\n",
    "for word in bidist.most_common(10):\n",
    "    print(word[0],\" \",word[1])\n",
    "print()\n",
    "print(f'The most common trigrams without removing stopwords are ')\n",
    "for word in tridist.most_common(10):\n",
    "    print(word[0],\" \",word[1])\n",
    "print()\n",
    "print(f'The most common fourgrams without removing stopwords are ')\n",
    "for word in fourdist.most_common(10):\n",
    "    print(word[0],\" \",word[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vARsvSfynePr"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/kanishk/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The most common unigrams after removing stopwords without smoothing are \n",
      "('the',)   1630\n",
      "('and',)   844\n",
      "('to',)   721\n",
      "('a',)   627\n",
      "('she',)   537\n",
      "('it',)   526\n",
      "('of',)   508\n",
      "('said',)   462\n",
      "('i',)   400\n",
      "('alice',)   385\n",
      "The most common bigrams after removing stopwords without smoothing are \n",
      "('said', 'the')   209\n",
      "('said', 'alice')   115\n",
      "('the', 'queen')   65\n",
      "('the', 'king')   60\n",
      "('a', 'little')   59\n",
      "('mock', 'turtle')   54\n",
      "('the', 'mock')   53\n",
      "('the', 'gryphon')   53\n",
      "('the', 'hatter')   51\n",
      "('went', 'on')   48\n",
      "\n",
      "The most common trigrams after removing stopwords without smoothing are \n",
      "('the', 'mock', 'turtle')   51\n",
      "('the', 'march', 'hare')   30\n",
      "('said', 'the', 'king')   29\n",
      "('the', 'white', 'rabbit')   21\n",
      "('said', 'the', 'hatter')   21\n",
      "('said', 'to', 'herself')   19\n",
      "('said', 'the', 'mock')   19\n",
      "('said', 'the', 'caterpillar')   18\n",
      "('she', 'went', 'on')   17\n",
      "('she', 'said', 'to')   17\n",
      "\n",
      "The most common fourgrams after removing stopwords without smoothing are \n",
      "('said', 'the', 'mock', 'turtle')   19\n",
      "('she', 'said', 'to', 'herself')   16\n",
      "('a', 'minute', 'or', 'two')   11\n",
      "('said', 'the', 'march', 'hare')   8\n",
      "('will', 'you', 'wont', 'you')   8\n",
      "('said', 'alice', 'in', 'a')   7\n",
      "('as', 'well', 'as', 'she')   6\n",
      "('well', 'as', 'she', 'could')   6\n",
      "('in', 'a', 'great', 'hurry')   6\n",
      "('in', 'a', 'tone', 'of')   6\n"
     ]
    }
   ],
   "source": [
    "#stopwords = code for downloading stop words through nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords \n",
    "stopword = stopwords.words('english')\n",
    "\n",
    "#print top 10 unigrams, bigrams after removing stopwords\n",
    "uni_processed = [p for p in unigrams if p not in stopword]\n",
    "fdist = nltk.FreqDist(uni_processed)\n",
    "\n",
    "bi_processed = [p for p in bigrams if (p[0] not in stopword) or (p[1] not in stopword)]\n",
    "bidist = nltk.FreqDist(bi_processed)\n",
    "\n",
    "tri_processed = [p for p in trigrams if (p[0] not in stopword) or (p[1] not in stopword) or (p[2] not in stopword)]\n",
    "tridist = nltk.FreqDist(tri_processed)\n",
    "\n",
    "four_processed = [p for p in fourgrams if (p[0] not in stopword) or (p[1] not in stopword) or (p[2] not in stopword) or (p[3] not in stopword)]\n",
    "fourdist = nltk.FreqDist(four_processed)\n",
    "\n",
    "\n",
    "#print top 10 bigrams, trigrams, fourgrams after removing stopwords\n",
    "print('The most common unigrams after removing stopwords without smoothing are ')\n",
    "for word in fdist.most_common(10):\n",
    "    print(word[0],\" \",word[1])\n",
    "\n",
    "print('The most common bigrams after removing stopwords without smoothing are ')\n",
    "for word in bidist.most_common(10):\n",
    "    print(word[0],\" \",word[1])\n",
    "print()\n",
    "print(f'The most common trigrams after removing stopwords without smoothing are ')\n",
    "for word in tridist.most_common(10):\n",
    "    print(word[0],\" \",word[1])\n",
    "print()\n",
    "print(f'The most common fourgrams after removing stopwords without smoothing are ')\n",
    "for word in fourdist.most_common(10):\n",
    "    print(word[0],\" \",word[1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ioc1xNjmnim-"
   },
   "source": [
    "# Applying Smoothing\n",
    "\n",
    "\n",
    "Assume additional training data in which each possible N-gram occurs exactly once and adjust estimates.\n",
    "\n",
    "> ### $ Probability(ngram) = \\frac{Count(ngram)+1}{ N\\, +\\, V} $\n",
    "\n",
    "N: Total number of N-grams <br>\n",
    "V: Number of unique N-grams\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smoothing(grams):\n",
    "    n = len(grams)\n",
    "    uniq = set(grams)\n",
    "    v = len(uniq)\n",
    "    fdist = nltk.FreqDist(grams)\n",
    "    fdist_smooth = nltk.FreqDist(grams)\n",
    "    for num in uniq :\n",
    "        c = fdist[num]\n",
    "        fdist_smooth[num] = (c+1)/(n+v)\n",
    "    return fdist_smooth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "grh4sO0Yns4V"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 unigrams after smoothing are : \n",
      "('the',)\n",
      "('and',)\n",
      "('to',)\n",
      "('a',)\n",
      "('she',)\n",
      "('it',)\n",
      "('of',)\n",
      "('said',)\n",
      "('i',)\n",
      "('alice',)\n",
      "\n",
      "Top 10 bigrams after smoothing are : \n",
      "('said', 'the')\n",
      "('of', 'the')\n",
      "('said', 'alice')\n",
      "('in', 'a')\n",
      "('and', 'the')\n",
      "('in', 'the')\n",
      "('it', 'was')\n",
      "('to', 'the')\n",
      "('the', 'queen')\n",
      "('as', 'she')\n",
      "\n",
      "Top 10 trigrams after smoothing are : \n",
      "('the', 'mock', 'turtle')\n",
      "('the', 'march', 'hare')\n",
      "('said', 'the', 'king')\n",
      "('the', 'white', 'rabbit')\n",
      "('said', 'the', 'hatter')\n",
      "('said', 'to', 'herself')\n",
      "('said', 'the', 'mock')\n",
      "('said', 'the', 'caterpillar')\n",
      "('she', 'went', 'on')\n",
      "('she', 'said', 'to')\n",
      "\n",
      "Top 10 fourgrams after smoothing are : \n",
      "('said', 'the', 'mock', 'turtle')\n",
      "('she', 'said', 'to', 'herself')\n",
      "('a', 'minute', 'or', 'two')\n",
      "('said', 'the', 'march', 'hare')\n",
      "('will', 'you', 'wont', 'you')\n",
      "('said', 'alice', 'in', 'a')\n",
      "('as', 'well', 'as', 'she')\n",
      "('well', 'as', 'she', 'could')\n",
      "('in', 'a', 'great', 'hurry')\n",
      "('in', 'a', 'tone', 'of')\n"
     ]
    }
   ],
   "source": [
    "#You are to perform Add-1 smoothing here:\n",
    "#write similar code for bigram, trigram and fourgrams\n",
    "\n",
    "#Print top 10 unigram, bigram, trigram, fourgram after smoothing\n",
    "#You are to perform Add-1 smoothing here:\n",
    "#write similar code for bigram, trigram and fourgrams\n",
    "#print top 10 unigrams, bigrams without removing stopwords\n",
    "fdist_smooth_u = smoothing(unigrams)\n",
    "print(\"Top 10 unigrams after smoothing are : \")\n",
    "for uni in fdist_smooth_u.most_common(10) : \n",
    "    print(uni[0])\n",
    "print(\"\")\n",
    "fdist_smooth_b = smoothing(bigrams)\n",
    "print(\"Top 10 bigrams after smoothing are : \")\n",
    "for bi in fdist_smooth_b.most_common(10) : \n",
    "    print(bi[0])\n",
    "print(\"\")\n",
    "fdist_smooth_t = smoothing(trigrams)\n",
    "print(\"Top 10 trigrams after smoothing are : \")\n",
    "for trig in fdist_smooth_t.most_common(10) : \n",
    "    print(trig[0])\n",
    "print(\"\")\n",
    "fdist_smooth_f = smoothing(fourgrams)\n",
    "print(\"Top 10 fourgrams after smoothing are : \")\n",
    "for fourg in fdist_smooth_f.most_common(10) : \n",
    "    print(fourg[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "k0GL40mQmmt4"
   },
   "source": [
    "### Predict the next word using statistical language modelling\n",
    "\n",
    "Using the above bigram, trigram, and fourgram models that you just experimented with, **predict the next word(top 5 probable) given the previous n(=2, 3, 4)-grams** for the sentences below.\n",
    "\n",
    "For str1, str2, you are to predict the next  2 possible word sequences using your trained smoothed models. <br> \n",
    "For example, for the string 'He looked very' the answers can be as below: \n",
    ">     (1) 'He looked very' *anxiouxly* \n",
    ">     (2) 'He looked very' *uncomfortable* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MBWKo5_Fmnbg"
   },
   "outputs": [],
   "source": [
    "str1 = 'after that alice said the'\n",
    "str2 = 'alice felt so desperate that she was'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ext_nVn2mvZt"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 most probable Predictions for bigram model For str1 : \n",
      "['queen', 'king', 'mock', 'gryphon', 'hatter']\n",
      "5 most probable Predictions for bigram model For str2 : \n",
      "['a', 'the', 'not', 'going', 'that']\n",
      "5 most Predictions for Trigram model For str1 : \n",
      "['king', 'hatter', 'mock', 'caterpillar', 'gryphon']\n",
      "5 most Predictions for Trigram model For str2 : \n",
      "['now', 'quite', 'a', 'walking', 'looking']\n",
      "5 most Predictions for Fourgram model For str1 : \n",
      "[]\n",
      "5 most Predictions for Fourgram model For str2 : \n",
      "['now', 'dozing', 'walking', 'ready', 'in']\n"
     ]
    }
   ],
   "source": [
    "pred_b_1 = []\n",
    "pred_b_2 = []\n",
    "pred_t_1 = []\n",
    "pred_t_2 = []\n",
    "pred_f_1 = []\n",
    "pred_f_2 = []\n",
    "words1 = word_tokenize(str1)\n",
    "words2 = word_tokenize(str2)\n",
    "num = 5\n",
    "\n",
    "cnt = 0\n",
    "for big in fdist_smooth_b.most_common():\n",
    "    if(big[0][0]==words1[-1]):\n",
    "        pred_b_1.append(big[0][1])\n",
    "        cnt+=1\n",
    "    if(cnt==num):\n",
    "        break;\n",
    "\n",
    "cnt = 0\n",
    "for big in fdist_smooth_b.most_common():\n",
    "    if(big[0][0]==words2[-1]):\n",
    "        pred_b_2.append(big[0][1])\n",
    "        cnt+=1\n",
    "    if(cnt==num):\n",
    "        break;\n",
    "\n",
    "print(\"5 most probable Predictions for bigram model For str1 : \")\n",
    "print(pred_b_1)\n",
    "print(\"5 most probable Predictions for bigram model For str2 : \")\n",
    "print(pred_b_2)\n",
    "\n",
    "cnt = 0\n",
    "for trig in fdist_smooth_t.most_common():\n",
    "    if(trig[0][0]==words1[-2] and trig[0][1]==words1[-1]):\n",
    "        pred_t_1.append(trig[0][2])\n",
    "        cnt+=1\n",
    "    if(cnt==num):\n",
    "        break;\n",
    "\n",
    "cnt = 0\n",
    "for trig in fdist_smooth_t.most_common():\n",
    "    if(trig[0][0]==words2[-2] and trig[0][1]==words2[-1]):\n",
    "        pred_t_2.append(trig[0][2])\n",
    "        cnt+=1\n",
    "    if(cnt==num):\n",
    "        break;\n",
    "\n",
    "print(\"5 most Predictions for Trigram model For str1 : \")\n",
    "print(pred_t_1)\n",
    "print(\"5 most Predictions for Trigram model For str2 : \")\n",
    "print(pred_t_2)\n",
    "\n",
    "\n",
    "cnt = 0\n",
    "for fourg in fdist_smooth_f.most_common():\n",
    "    if(fourg[0][0]==words1[-3] and fourg[0][1]==words1[-2] and fourg[0][2]==words1[-1]):\n",
    "        pred_f_1.append(fourg[0][3])\n",
    "        cnt+=1\n",
    "    if(cnt==num):\n",
    "        break;\n",
    "\n",
    "cnt = 0\n",
    "for fourg in fdist_smooth_f.most_common():\n",
    "    if(fourg[0][0]==words2[-3] and fourg[0][1]==words2[-2] and fourg[0][2]==words2[-1]):\n",
    "        pred_f_2.append(fourg[0][3])\n",
    "        cnt+=1\n",
    "    if(cnt==num):\n",
    "        break;\n",
    "\n",
    "\n",
    "print(\"5 most Predictions for Fourgram model For str1 : \")\n",
    "print(pred_f_1)\n",
    "print(\"5 most Predictions for Fourgram model For str2 : \")\n",
    "print(pred_f_2)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "NLP_Assignment_2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
