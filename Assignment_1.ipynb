{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fk0evCZ7U9WO"
   },
   "source": [
    "# **Assignment 1 on Natural Language Processing**\n",
    "\n",
    "### Date : 4th Sept, 2020\n",
    "\n",
    "#### Instructor : Prof. Sudeshna Sarkar\n",
    "\n",
    "#### Teaching Assistants : Alapan Kuila, Aniruddha Roy, Anusha Potnuru, Uppada Vishnu\n",
    "\n",
    "### Name:   Kanishk Singh\n",
    "\n",
    "### RollNo: 17CS30018  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Il_b_LFKXi8t"
   },
   "source": [
    " # NLTK Library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ss5CZjC2Xt0i"
   },
   "source": [
    "The [NLTK](https://www.nltk.org/) Python framework is generally used as an education and research tool. Tokenization, Stemming, Lemmatization, Punctuation, Character count, word count are some of these packages which will be discussed in this tutorial.\n",
    "\n",
    "**Installing Nltk** <br>\n",
    "Nltk can be installed using PIP or Conda package managers.For detailed installation instructions follow this [link](https://www.nltk.org/install.html).\n",
    "\n",
    "To ensure we are all on the same page, the coding environment will be in **python3**. We suggest downloading Anaconda3 and creating a separate environment to do this assignment. \n",
    "The link to anaconda3 for Windows and Linux is available here https://docs.anaconda.com/anaconda/install/. \n",
    "The steps to install NLTK is available on the link: \n",
    "```bash\n",
    "sudo pip3 install nltk \n",
    "python3 \n",
    "nltk.download()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "r4txbU5-RlMv"
   },
   "source": [
    "**Note for Question and answers:**\n",
    "\n",
    "Write your answers to the point in the text box below labelled as **Answer here**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "52_aJRSqaHgC"
   },
   "source": [
    "# Tokenizing words and Sentences using Nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "o5_ElYaeaMbR"
   },
   "source": [
    "**Tokenization** is the process by which big quantity of text is divided into smaller parts called tokens. <br>It is crucial to understand the pattern in the text in order to perform various NLP tasks.These tokens are very useful for finding such patterns.<br>\n",
    "\n",
    "Natural Language toolkit has very important module tokenize which further comprises of sub-modules\n",
    "\n",
    "1. word tokenize\n",
    "2. sentence tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sby_OS3qZ_fz"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/kanishk/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package inaugural to\n",
      "[nltk_data]     /home/kanishk/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/inaugural.zip.\n"
     ]
    }
   ],
   "source": [
    "# Importing modules\n",
    "import nltk\n",
    "nltk.download('punkt') # For tokenizers\n",
    "nltk.download('inaugural') # For dataset\n",
    "from nltk.tokenize import word_tokenize,sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ew9Aq5WHXSn-"
   },
   "outputs": [],
   "source": [
    "# Sample corpus.\n",
    "from nltk.corpus import inaugural\n",
    "corpus = inaugural.raw('1789-Washington.txt')\n",
    "from nltk.corpus import stopwords \n",
    "#print(corpus)\n",
    "#print(type(corpus))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "V2wbXKzVW0GO"
   },
   "source": [
    "### **TASK**:\n",
    "\n",
    "For the given corpus, \n",
    "1. Print the number of sentences and tokens. \n",
    "2. Print the average number of tokens per sentence.\n",
    "3. Print the number of unique tokens\n",
    "4. Print the number of tokens after stopword removal using the stopwords from nltk.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jrtu9HcHXFe6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens =  1537 and Number of sentences =  23\n",
      "Average number of tokens per sentence =  67\n",
      "Number of Unique tokens are =  626\n",
      "Number of tokens after stopword removal using stopwords from nltk are =  800\n"
     ]
    }
   ],
   "source": [
    "# TODO\n",
    "word_tokens = word_tokenize(corpus)\n",
    "sent_tokens = sent_tokenize(corpus)\n",
    "unique_tokens = set(word_tokens)\n",
    "print(\"Number of tokens = \",len(word_tokens), \"and Number of sentences = \", len(sent_tokens))\n",
    "\n",
    "num_words = len(word_tokens)\n",
    "num_sents = len(sent_tokens)\n",
    "print(\"Average number of tokens per sentence = \", round(num_words/num_sents))\n",
    "\n",
    "print(\"Number of Unique tokens are = \", len(unique_tokens))\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "filtered_sentence = [w for w in word_tokens if not w in stop_words]\n",
    "print(\"Number of tokens after stopword removal using stopwords from nltk are = \",len(filtered_sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UViYY9_3t2UE"
   },
   "source": [
    "# Stemming and Lemmatization with NLTK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "g55XX9KDLgO7"
   },
   "source": [
    "**What is Stemming?** <br>\n",
    "Stemming is a kind of normalization for words. Normalization is a technique where a set of words in a sentence are converted into a sequence to shorten its lookup. The words which have the same meaning but have some variation according to the context or sentence are normalized.<br>\n",
    "Hence Stemming is a way to find the root word from any variations of respective word\n",
    "\n",
    "There are many stemmers provided by Nltk like **PorterStemmer**, **SnowballStemmer**, **LancasterStemmer**.<br>\n",
    "\n",
    "We will try and see differences between Porterstemmer and Snowballstemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SS4Ij__XLfTB"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stemmed version by PorterStemmer: \n",
      "grows  and  grow\n",
      "leaves  and  leav\n",
      "fairly  and  fairli\n",
      "cats  and  cat\n",
      "trouble  and  troubl\n",
      "misunderstanding  and  misunderstand\n",
      "friendships  and  friendship\n",
      "easily  and  easili\n",
      "rational  and  ration\n",
      "relational  and  relat\n",
      "\n",
      "Stemmed version by SnowballStemmer: \n",
      "grows  and  grow\n",
      "leaves  and  leav\n",
      "fairly  and  fair\n",
      "cats  and  cat\n",
      "trouble  and  troubl\n",
      "misunderstanding  and  misunderstand\n",
      "friendships  and  friendship\n",
      "easily  and  easili\n",
      "rational  and  ration\n",
      "relational  and  relat\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import SnowballStemmer # Note that SnowballStemmer has language as parameter.\n",
    "\n",
    "words = [\"grows\",\"leaves\",\"fairly\",\"cats\",\"trouble\",\"misunderstanding\",\"friendships\",\"easily\", \"rational\", \"relational\"]\n",
    "\n",
    "# TODO\n",
    "# create an instance of both the stemmers and perform stemming on above words\n",
    "\n",
    "ps = PorterStemmer()\n",
    "ss = SnowballStemmer(\"english\")\n",
    "print(\"Stemmed version by PorterStemmer: \")\n",
    "for w in words:\n",
    "    print(w,\" and \", ps.stem(w))\n",
    "\n",
    "print(\"\")\n",
    "\n",
    "print(\"Stemmed version by SnowballStemmer: \")\n",
    "for w in words:\n",
    "    print(w,\" and \", ss.stem(w))\n",
    "\n",
    "\n",
    "# TODO\n",
    "# Complete the function which takes a sentence/corpus and gets its stemmed version.\n",
    "def stemSentence(sentence=None):\n",
    "    filtered_sentence = [ss.stem(w) for w in sentence]\n",
    "    return filtered_sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0JuE8CuDQSno"
   },
   "source": [
    "**What is Lemmatization?** <br>\n",
    "Lemmatization is the algorithmic process of finding the lemma of a word depending on their meaning. Lemmatization usually refers to the morphological analysis of words, which aims to remove inflectional endings. It helps in returning the base or dictionary form of a word, which is known as the lemma.<br>\n",
    "\n",
    "*The NLTK Lemmatization method is based on WorldNet's built-in morph function.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "noyl1YNsQp98"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grows  adjective  grows\n",
      "grows  verb       grow\n",
      "grows  noun       grows\n",
      "leaves  adjective  leaves\n",
      "leaves  verb       leave\n",
      "leaves  noun       leaf\n",
      "fairly  adjective  fairly\n",
      "fairly  verb       fairly\n",
      "fairly  noun       fairly\n",
      "cats  adjective  cats\n",
      "cats  verb       cat\n",
      "cats  noun       cat\n",
      "trouble  adjective  trouble\n",
      "trouble  verb       trouble\n",
      "trouble  noun       trouble\n",
      "running  adjective  running\n",
      "running  verb       run\n",
      "running  noun       running\n",
      "friendships  adjective  friendships\n",
      "friendships  verb       friendships\n",
      "friendships  noun       friendship\n",
      "easily  adjective  easily\n",
      "easily  verb       easily\n",
      "easily  noun       easily\n",
      "was  adjective  was\n",
      "was  verb       be\n",
      "was  noun       wa\n",
      "relational  adjective  relational\n",
      "relational  verb       relational\n",
      "relational  noun       relational\n",
      "has  adjective  has\n",
      "has  verb       have\n",
      "has  noun       ha\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/kanishk/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#imports\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet') # Since Lemmatization method is based on WorldNet's built-in morph function.\n",
    "\n",
    "words = [\"grows\",\"leaves\",\"fairly\",\"cats\",\"trouble\",\"running\",\"friendships\",\"easily\", \"was\", \"relational\",\"has\"]\n",
    "\n",
    "#TODO\n",
    "# Create an instance of the Lemmatizer and perform Lemmatization on above words\n",
    "# You can also give Parts-of-speech(pos) to the Lemmatizer for example \"v\" (verb). Check the differences in the outputs.\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "for w in words:\n",
    "    print(w,\" adjective \", lemmatizer.lemmatize(w,\"a\"))\n",
    "    print(w,\" verb      \", lemmatizer.lemmatize(w,\"v\"))\n",
    "    print(w,\" noun      \", lemmatizer.lemmatize(w,\"n\"))\n",
    "\n",
    "\n",
    "#TODO\n",
    "# Complete the function which takes a sentence/corpus and gets its lemmatized version.\n",
    "def lemmatizeSentence(sentence=None):\n",
    "    filtered_sentence = [lemmatizer.lemmatize(w,\"n\") for w in sentence]\n",
    "    return filtered_sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VJW6HsycSAlU"
   },
   "source": [
    "**Question:** Give example of two words which have same stem but different lemma? Show the stem and lemma of both words in the code below \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zcq6bUEaSAt1"
   },
   "source": [
    "**Answer here:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "a8OtIEmFkGBM"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stem of the above two words are  ['solv', 'solv']\n",
      "lemma of the above two words are  ['solved', 'solving']\n"
     ]
    }
   ],
   "source": [
    "#TODO\n",
    "# Write code to print the stem and lemma of both your words\n",
    "words = ['solved','solving']\n",
    "print(\"stem of the above two words are \",stemSentence(words))\n",
    "print(\"lemma of the above two words are \",lemmatizeSentence(words))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "e0tz3SIGSA2b"
   },
   "source": [
    "**Question:** Write a comparison between stemming and lemmatization?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aks8QaQ_SA_N"
   },
   "source": [
    "**Stemming algorithms** work by cutting off the end or the beginning of the word, taking into account a list of common prefixes and suffixes that can be found in an inflected word. This indiscriminate cutting can be successful in some occasions, but not always, and that is why we affirm that this approach presents some limitations.\n",
    "\n",
    "**Lemmatization**, on the other hand, takes into consideration the morphological analysis of the words. To do so, it is necessary to have detailed dictionaries which the algorithm can look through to link the form back to its lemma.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "NLP Assignment 1",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
